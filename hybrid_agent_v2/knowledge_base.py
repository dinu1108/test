import chromadb
from faster_whisper import WhisperModel
import os
import re
import subprocess
import numpy as np
import chromadb.utils.embedding_functions as ef_functions
from pathlib import Path
from tqdm import tqdm
from config import load_api_key

class VideoKnowledgeBase:
    def __init__(self, collection_name="video_memory"):
        load_api_key() # API í‚¤ ë¡œë“œ (Whisper ë“± ë‹¤ë¥¸ ìš©ë„ ìœ„í•´ ìœ ì§€)
        
        self.persist_dir = Path("./hybrid_agent_v2/chroma_db")
        self.persist_dir.mkdir(exist_ok=True, parents=True)
        
        # [ë³€ê²½] êµ¬ê¸€ API ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸(SentenceTransformer) ì‚¬ìš©
        # ì´ ëª¨ë¸ì€ ì‚¬ìš©ìë‹˜ì˜ ì»´í“¨í„°ì—ì„œ ì§ì ‘ ì‹¤í–‰ë˜ì–´ í• ë‹¹ëŸ‰ ì œí•œì´ ì—†ìŠµë‹ˆë‹¤.
        print("[KnowledgeBase] ğŸ“¥ Loading Local Embedding Model (all-MiniLM-L6-v2)...")
        self.embedding_function = ef_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        self.client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # [ì£¼ì˜] ì„ë² ë”© ëª¨ë¸ì´ ë°”ë€Œë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë ¤ë©´ ë³„ë„ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ í•„ìš”í•˜ë‚˜, Factory ëª¨ë“œ íŠ¹ì„±ìƒ
        # ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“œëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” get_or_createë¡œ í•˜ë˜, ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë„ìš°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        
        self.log_file = self.persist_dir / "rejection_logs.jsonl" # ì‹¤íŒ¨ ë¡œê·¸ íŒŒì¼
        
        # Initialize Collection
        self.whisper_model = None
        self.init_collection(collection_name)
        print(f"[KnowledgeBase] ğŸ  Local Embedding Mode (Unlimited) Ready.")

    def log_rejection(self, video_id, candidate_data, reason, final_score):
        """íƒˆë½í•œ í›„ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡ (ìë™ íŠœë‹ìš© Seed + ìë™ ë¶„ë¥˜)"""
        import json
        from datetime import datetime
        
        # âœ… FIX #10: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìë™ ë¶„ë¥˜
        classification = "other"
        r_lower = reason.lower()
        if "low score" in r_lower: classification = "low_quality"
        elif "unnecessary" in r_lower: classification = "is_boring"
        elif "context" in r_lower: classification = "context_missing"
        elif "payoff" in r_lower: classification = "low_payoff"

        entry = {
            "timestamp": datetime.now().isoformat(),
            "video_id": video_id,
            "category": classification,
            "candidate": candidate_data,
            "reason": reason,
            "final_score": final_score
        }
        
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"[KB] âš ï¸ Log Error: {e}")

    def init_collection(self, collection_name):
        try:
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
        except Exception as e:
            print(f"[KB] âš ï¸ Collection Error: {e}")
            print("[KB] ê¸°ì¡´ DBì™€ ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'hybrid_agent_v2/chroma_db' í´ë”ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            raise e

    def _load_whisper(self, model_size=None):
        if model_size is None:
            from presets.factory_config import FactoryConfig
            model_size = getattr(FactoryConfig, 'WHISPER_MODEL', "small")

        # ê¸°ì¡´ ë¡œë“œëœ ëª¨ë¸ê³¼ ì‚¬ì´ì¦ˆê°€ ë‹¤ë¥´ë©´ ìƒˆë¡œ ë¡œë“œ
        if self.whisper_model and getattr(self, '_current_model_size', None) == model_size:
            return
            
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"
        
        print(f"[KnowledgeBase] ğŸ¤– Loading Whisper Model ({model_size}) on {device.upper()} ({compute_type})...")
        self.whisper_model = WhisperModel(model_size, device=device, compute_type=compute_type)
        self._current_model_size = model_size

    def extract_audio(self, video_path):
        # [Safety Check] (Preserved from previous fix)
        video_path = Path(video_path)
        if not video_path.exists():
            potential_path = Path("raw_data") / video_path.name
            if potential_path.exists():
                print(f"[KB] ğŸ“ Found prompt file in raw_data: {potential_path}")
                video_path = potential_path
        
        # [User Request] Absolute path & Verbose Debugging
        video_path_obj = video_path.absolute() # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
        audio_path = video_path_obj.with_suffix(".wav")
        
        if not audio_path.exists():
            print(f"[KnowledgeBase] Extracting audio from {video_path_obj.name}...")
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì¸ìëŠ” ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, 
            # ìœˆë„ìš°ì—ì„œëŠ” shell=Trueì™€ í•¨ê»˜ ë¬¸ìì—´ë¡œ ì£¼ëŠ” ê²ƒì´ ë” ì•ˆì „í•  ë•Œê°€ ìˆìŠµë‹ˆë‹¤.
            cmd = [
                "ffmpeg", "-y", "-nostdin",
                "-i", str(video_path_obj),
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                str(audio_path)
            ]
            try:
                # stderrë¥¼ DEVNULLë¡œ ë³´ë‚´ì§€ ë§ê³  ì¶œë ¥í•˜ê²Œ í•˜ì—¬ ì—ëŸ¬ ì›ì¸ì„ í™•ì¸í•©ë‹ˆë‹¤.
                subprocess.run(cmd, check=True)
            except subprocess.CalledProcessError as e:
                print(f"âŒ FFmpeg ì—ëŸ¬ ë°œìƒ! íŒŒì¼ëª…ì— íŠ¹ìˆ˜ë¬¸ìê°€ ìˆëŠ”ì§€, í˜¹ì€ ffmpegê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                raise e
        return str(audio_path)

    def ingest(self, video_path):
        """ì˜ìƒ ì „ì‚¬ ë° ë²¡í„° DB ì¸ë±ì‹± (ëŒ€ìš©ëŸ‰ ì²­í¬ ì²˜ë¦¬ + í™˜ê° í•„í„°)"""
        import subprocess, json, tempfile, os
        try:
            import psutil
        except ImportError:
            psutil = None
        from pathlib import Path
        video_id = Path(video_path).stem
        existing = self.collection.get(where={"video_id": video_id}, limit=1)
        if existing['ids']:
            print(f"[KnowledgeBase] Video '{video_id}' already indexed. Skipping.")
            return

        # Whisper ëª¨ë¸ ë¡œë“œ
        self._load_whisper()

        # ---- í—¬í¼ í•¨ìˆ˜ë“¤ ----
        def _get_video_duration(v_path):
            """ffprobe ë¡œ ì „ì²´ ì˜ìƒ ê¸¸ì´(ì´ˆ) ë°˜í™˜"""
            cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "json", str(v_path)]
            result = subprocess.run(cmd, capture_output=True, text=True)
            info = json.loads(result.stdout)
            return float(info["format"]["duration"])

        def _extract_chunk_audio(v_path, start_sec, end_sec):
            """êµ¬ê°„ ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ wav íŒŒì¼ë¡œ ì¶”ì¶œ"""
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            tmp_path = tmp.name
            tmp.close()
            cmd = [
                "ffmpeg", "-y", "-nostdin",
                "-ss", str(start_sec),
                "-to", str(end_sec),
                "-i", str(v_path),
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                tmp_path
            ]
            subprocess.run(cmd, check=True)
            return tmp_path

        def _filter_hallucination(text):
            """ê¸¸ì´ <=2 í† í°ì´ 5ë²ˆ ì´ìƒ ì—°ì†ë  ê²½ìš° ì œê±°"""
            tokens = text.split()
            filtered = []
            i = 0
            while i < len(tokens):
                token = tokens[i]
                if len(token) <= 2:
                    cnt = 1
                    j = i + 1
                    while j < len(tokens) and tokens[j] == token:
                        cnt += 1
                        j += 1
                    if cnt < 5:
                        filtered.extend(tokens[i:j])
                    i = j
                    continue
                filtered.append(token)
                i += 1
            return " ".join(filtered)
        # --------------------------

        total_seconds = _get_video_duration(video_path)
        chunk_sec = 1800  # 30ë¶„ ì²­í¬ (I/Oì™€ ë©”ëª¨ë¦¬ ê· í˜•)
        start = 0
        chunk_idx = 0
        all_segments = []
        
        # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì˜ìƒ ì²˜ë¦¬ ì§„í–‰ë¥  í‘œì‹œ
        pbar = tqdm(total=total_seconds, desc="[KnowledgeBase] Transcribing Audio", unit="s")
        
        while start < total_seconds:
            chunk_idx += 1
            end = min(start + chunk_sec, total_seconds)
            mem_usage = f"{psutil.virtual_memory().percent}%" if psutil else "N/A"
            print(f"[KnowledgeBase] Processing chunk {chunk_idx}: {start:.0f}s â€“ {end:.0f}s (Memory usage: {mem_usage})")
            chunk_audio_path = _extract_chunk_audio(video_path, start, end)
            
            try:
                # faster-whisperëŠ” ì œë„ˆë ˆì´í„° ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                segments, info = self.whisper_model.transcribe(
                    chunk_audio_path, 
                    language="ko", 
                    beam_size=5,
                    vad_filter=True, # ì¹¨ë¬µ êµ¬ê°„ ìë™ ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ í–¥ìƒ)
                    word_timestamps=False
                )
                
                print(f"   ğŸš€ [Faster-Whisper] Detected language '{info.language}' with probability {info.language_probability:.2f}")
                
                for seg in segments:
                    # faster-whisper segment ê°ì²´ëŠ” start, end, text ì†ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.
                    all_segments.append({
                        "start": seg.start + start,
                        "end": seg.end + start,
                        "text": _filter_hallucination(seg.text)
                    })
                    if len(all_segments) % 50 == 0:
                        pbar.set_postfix({"time": f"{seg.start + start:.0f}s"})
                        
                start = end
                pbar.update(chunk_sec)
                
            finally:
                if os.path.exists(chunk_audio_path):
                    os.remove(chunk_audio_path)
        
        pbar.close()

        # ---- DB ì¸ë±ì‹± (V5: ë©”íƒ€ë°ì´í„° ê°•í™”) ----
        ids, docs, metadatas = [], [], []
        last_segment_end = 0
        
        for i, seg in enumerate(all_segments):
            txt = seg["text"].strip()
            if len(txt) < 2:
                continue
            
            # (V5) ë©”íƒ€ë°ì´í„° ì‚°ì¶œ
            duration = max(0.1, float(seg["end"]) - float(seg["start"]))
            speech_density = len(txt) / (duration / 60) # Chars per minute
            silence_gap = (float(seg["start"]) - last_segment_end) if last_segment_end > 0 else 0
            
            ids.append(f"{video_id}_{i}")
            docs.append(txt)
            metadatas.append({
                "video_id": video_id,
                "start": float(seg["start"]),
                "end": float(seg["end"]),
                "speech_density": float(speech_density),
                "silence_gap": float(silence_gap),
                "is_gap_start": 1 if silence_gap > 15 else 0
            })
            
            last_segment_end = float(seg["end"])

            if len(ids) >= 10:
                self._add_batch(ids, docs, metadatas)
                ids, docs, metadatas = [], [], []
        if ids:
            self._add_batch(ids, docs, metadatas)

        print(f"[KnowledgeBase] Ingest Complete for {video_id}. Processed {chunk_idx} chunk(s).")

    def _add_batch(self, ids, docs, metadatas):
        import time
        max_retries = 5
        base_delay = 10
        
        for attempt in range(max_retries):
            try:
                self.collection.add(documents=docs, metadatas=metadatas, ids=ids)
                return # Success
            except Exception as e:
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    delay = base_delay * (attempt + 1)
                    print(f"\n[KB] âš ï¸ Rate Limit (429) hit. Sleeping for {delay}s... (Attempt {attempt+1}/{max_retries})")
                    time.sleep(delay)
                else:
                    print(f"\n[KB] âŒ DB Insert Error: {e}")
                    raise e
        print("\n[KB] âŒ Failed to insert batch after retries due to Rate Limits.")
        raise RuntimeError("ChromaDB Insert Failed: Rate Limit Exceeded")

    def get_context(self, video_id, start_time, end_time):
        """íŠ¹ì • ì‹œê°„ ë²”ìœ„ ë‚´ì˜ ëŒ€ë³¸ ì¶”ì¶œ (ìˆ˜ì • ì™„ë£Œ)"""
        results = self.collection.get(
            where={
                "$and": [
                    {"video_id": video_id},
                    {"start": {"$gte": float(start_time)}},
                    {"start": {"$lte": float(end_time)}}
                ]
            }
        )
        
        segments = []
        if results['ids']:
            for i in range(len(results['ids'])):
                seg = results['metadatas'][i].copy()
                seg["text"] = results['documents'][i]
                segments.append(seg)
        return sorted(segments, key=lambda x: x['start'])

    def clean_text_for_llm(self, text):
        text = re.sub(r'\[.*?\]', '', text)
        return re.sub(r'\s+', ' ', text).strip()

    def get_optimized_transcript(self, video_path, threshold_percentile=80):
        """[Token Saver] V1 ì˜¤ë””ì˜¤ ë¶„ì„ê³¼ ì—°ë™í•˜ì—¬ ëŒ€ë³¸ ì••ì¶•"""
        # 1. ë¡œì»¬ V1 ë¶„ì„ê¸° ê°€ë™
        try:
            from modules.analyst import Analyst
        except ImportError:
            print("[KB] âš ï¸ V1 Analyst module not found. Returning full text.")
            return "V1 Analyst module error."

        print("[KB] ğŸ“‰ Running 'Token Saver' Pre-filtering (Audio Analysis)...")
        
        # Load Khan Preset Manually
        import json
        preset_path = Path("presets/Khan.json")
        if preset_path.exists():
            with open(preset_path, "r", encoding="utf-8") as f:
                chk_config = json.load(f)
        else:
            print("[KB] âš ï¸ 'presets/Khan.json' not found. Using default config.")
            chk_config = {}

        analyst = Analyst(config=chk_config)
        audio_data = analyst.analyze_audio_advanced(video_path)
        
        if not audio_data:
            print("[KB] âš ï¸ Audio analysis failed. Falling back to FULL transcript.")
            return self.get_full_transcript(video_path)
            
        scores, times = analyst.calculate_scores(audio_data)
        threshold = np.percentile(scores, threshold_percentile)
        active_indices = np.where(scores > threshold)[0]
        
        if len(active_indices) == 0: return "No active zones found."
            
        # 2. í™œì„± êµ¬ê°„ ë³‘í•© (Peak +/- 3ë¶„)
        active_times = times[active_indices]
        ranges = sorted([(max(0, t - 180), t + 180) for t in active_times])
        
        merged = []
        if ranges:
            curr_start, curr_end = ranges[0]
            for start, end in ranges[1:]:
                if start < curr_end:
                    curr_end = max(curr_end, end)
                else:
                    merged.append((curr_start, curr_end))
                    curr_start, curr_end = start, end
            merged.append((curr_start, curr_end))
            
        # 3. ë°ì´í„° ì¶”ì¶œ ë° 30ì´ˆ ë‹¨ìœ„ ë¸”ë¡í™”
        video_id = Path(video_path).stem
        optimized_lines = []
        
        for start, end in merged:
            segs = self.get_context(video_id, start, end)
            
            # 30ì´ˆ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¬¶ê¸° (Text Slimming)
            current_block_id = -1
            for s in segs:
                block_id = int(s['start'] // 30)
                clean_txt = self.clean_text_for_llm(s['text'])
                if not clean_txt: continue
                
                if block_id != current_block_id:
                    m, sec = divmod(block_id * 30, 60)
                    h, m = divmod(m, 60)
                    timestamp = f"[{int(h):02d}:{int(m):02d}:{int(sec):02d}]"
                    optimized_lines.append(f"\n{timestamp} {clean_txt}")
                    current_block_id = block_id
                else:
                    optimized_lines[-1] += f" {clean_txt}"
        
        return "".join(optimized_lines)

    def precise_retranscribe(self, video_path, clips):
        """
        [Stage 2.5] Golden Pass: ì„ ì •ëœ í›„ë³´ êµ¬ê°„ë§Œ Medium ëª¨ë¸ë¡œ ì •ë°€ ì „ì‚¬
        """
        if not clips: return clips
        print(f"\nğŸ’ [Golden Pass] Re-transcribing {len(clips)} highlights with MEDIUM model...")
        
        # ëª¨ë¸ í¬ì¸í„°ë¥¼ ë³´ì¡´í–ˆë‹¤ê°€ ë³µêµ¬í•  í•„ìš” ì—†ì´ í˜„ì‹œì ì—ì„œ ë¡œë“œ
        self._load_whisper(model_size="medium")
        
        import tempfile
        updated_count = 0
        for clip in clips:
            start = float(clip['start'])
            end = float(clip['end'])
            
            # êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ í—¬í¼ (ë‚´ë¶€ ì •ì˜)
            def _tmp_extract(v, s, e):
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
                tp = tmp.name; tmp.close()
                cmd = ["ffmpeg", "-y", "-nostdin", "-ss", str(s), "-to", str(e), "-i", str(v),
                       "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", tp]
                subprocess.run(cmd, capture_output=True)
                return tp

            audio_path = _tmp_extract(video_path, start, end)
            try:
                segments, _ = self.whisper_model.transcribe(audio_path, language="ko", beam_size=5)
                new_text = " ".join([s.text for s in segments]).strip()
                if new_text:
                    clip['text'] = f"[HQ] {new_text}" # HQ í‘œì‹œ
                    updated_count += 1
            except: pass
            finally:
                if os.path.exists(audio_path): os.remove(audio_path)
        
        print(f"ğŸ“Š [Golden Pass] Done. {updated_count} clips refined.")
        return clips

    def get_full_transcript(self, video_path):
        """ì €ì¥ëœ ëª¨ë“  ë¬¸ì¥ì„ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ ëŒ€ë³¸ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤ (Fallbackì „ìš©)"""
        video_id = Path(video_path).stem
        print(f"[KnowledgeBase] ğŸ“ Fetching all segments for {video_id}...")
        results = self.collection.get(
            where={"video_id": video_id}
        )
        
        if not results['ids']:
            return ""

        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        segments = []
        for i in range(len(results['ids'])):
            segments.append({
                "start": results['metadatas'][i]['start'],
                "text": results['documents'][i]
            })
        
        sorted_segs = sorted(segments, key=lambda x: x['start'])
        
        # 30ì´ˆ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ í…ìŠ¤íŠ¸ ì–‘ ìµœì í™”
        full_text = []
        current_block = -1
        for s in sorted_segs:
            block = int(s['start'] // 30)
            if block != current_block:
                m, sec = divmod(block * 30, 60)
                h, m = divmod(m, 60)
                full_text.append(f"\n[{int(h):02d}:{int(m):02d}:{int(sec):02d}] {s['text']}")
                current_block = block
            else:
                full_text[-1] += f" {s['text']}"
        
        return "".join(full_text)
