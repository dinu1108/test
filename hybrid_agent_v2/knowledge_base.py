import chromadb
import whisper
import os
import re
import subprocess
import numpy as np
import chromadb.utils.embedding_functions as ef_functions
from pathlib import Path
from config import load_api_key

class VideoKnowledgeBase:
    def __init__(self, collection_name="video_memory"):
        load_api_key() # API í‚¤ ë¡œë“œ (Whisper ë“± ë‹¤ë¥¸ ìš©ë„ ìœ„í•´ ìœ ì§€)
        
        self.persist_dir = Path("./hybrid_agent_v2/chroma_db")
        self.persist_dir.mkdir(exist_ok=True, parents=True)
        
        # [ë³€ê²½] êµ¬ê¸€ API ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸(SentenceTransformer) ì‚¬ìš©
        # ì´ ëª¨ë¸ì€ ì‚¬ìš©ìë‹˜ì˜ ì»´í“¨í„°ì—ì„œ ì§ì ‘ ì‹¤í–‰ë˜ì–´ í• ë‹¹ëŸ‰ ì œí•œì´ ì—†ìŠµë‹ˆë‹¤.
        print("[KnowledgeBase] ğŸ“¥ Loading Local Embedding Model (all-MiniLM-L6-v2)...")
        self.embedding_function = ef_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        self.client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # [ì£¼ì˜] ì„ë² ë”© ëª¨ë¸ì´ ë°”ë€Œë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë ¤ë©´ ë³„ë„ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ í•„ìš”í•˜ë‚˜, Factory ëª¨ë“œ íŠ¹ì„±ìƒ
        # ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“œëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” get_or_createë¡œ í•˜ë˜, ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë„ìš°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        
        self.log_file = self.persist_dir / "rejection_logs.jsonl" # ì‹¤íŒ¨ ë¡œê·¸ íŒŒì¼
        
        # Initialize Collection
        self.init_collection(collection_name)
        
        print(f"[KnowledgeBase] ğŸ  Local Embedding Mode (Unlimited) Ready.")
        self.whisper_model = None

    def log_rejection(self, video_id, candidate_data, reason, final_score):
        """íƒˆë½í•œ í›„ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡ (ìë™ íŠœë‹ìš© Seed)"""
        import json
        from datetime import datetime
        
        entry = {
            "timestamp": datetime.now().isoformat(),
            "video_id": video_id,
            "candidate": candidate_data,
            "reason": reason,
            "final_score": final_score
        }
        
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"[KB] âš ï¸ Log Error: {e}")

    def init_collection(self, collection_name):
        try:
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
        except Exception as e:
            print(f"[KB] âš ï¸ Collection Error: {e}")
            print("[KB] ê¸°ì¡´ DBì™€ ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'hybrid_agent_v2/chroma_db' í´ë”ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            raise e
        
        print(f"[KnowledgeBase] ğŸ  Local Embedding Mode (Unlimited) Ready.")
        self.whisper_model = None

    def _load_whisper(self):
        if not self.whisper_model:
            print("[KnowledgeBase] Loading Whisper Model (base)...")
            self.whisper_model = whisper.load_model("base")

    def extract_audio(self, video_path):
        # [Safety Check] (Preserved from previous fix)
        video_path = Path(video_path)
        if not video_path.exists():
            potential_path = Path("raw_data") / video_path.name
            if potential_path.exists():
                print(f"[KB] ğŸ“ Found prompt file in raw_data: {potential_path}")
                video_path = potential_path
        
        # [User Request] Absolute path & Verbose Debugging
        video_path_obj = video_path.absolute() # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
        audio_path = video_path_obj.with_suffix(".wav")
        
        if not audio_path.exists():
            print(f"[KnowledgeBase] Extracting audio from {video_path_obj.name}...")
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì¸ìëŠ” ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, 
            # ìœˆë„ìš°ì—ì„œëŠ” shell=Trueì™€ í•¨ê»˜ ë¬¸ìì—´ë¡œ ì£¼ëŠ” ê²ƒì´ ë” ì•ˆì „í•  ë•Œê°€ ìˆìŠµë‹ˆë‹¤.
            cmd = [
                "ffmpeg", "-y", "-i", str(video_path_obj),
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                str(audio_path)
            ]
            try:
                # stderrë¥¼ DEVNULLë¡œ ë³´ë‚´ì§€ ë§ê³  ì¶œë ¥í•˜ê²Œ í•˜ì—¬ ì—ëŸ¬ ì›ì¸ì„ í™•ì¸í•©ë‹ˆë‹¤.
                subprocess.run(cmd, check=True)
            except subprocess.CalledProcessError as e:
                print(f"âŒ FFmpeg ì—ëŸ¬ ë°œìƒ! íŒŒì¼ëª…ì— íŠ¹ìˆ˜ë¬¸ìê°€ ìˆëŠ”ì§€, í˜¹ì€ ffmpegê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                raise e
        return str(audio_path)

    def ingest(self, video_path):
        """ì˜ìƒì„ ì „ì‚¬í•˜ê³  ë²¡í„° DBì— ì¸ë±ì‹± (Rate Limit ëŒ€ì‘)"""
        import time
        video_id = Path(video_path).stem
        existing = self.collection.get(where={"video_id": video_id}, limit=1)
        if existing['ids']:
            print(f"[KnowledgeBase] Video '{video_id}' already indexed. Skipping.")
            return

        self._load_whisper()
        audio_path = self.extract_audio(video_path)
        
        print(f"[KnowledgeBase] Transcribing '{video_id}'... (Outputting logs for progress)")
        # Whisper Python API doesn't have a native progress bar, using verbose=True to show activity.
        result = self.whisper_model.transcribe(audio_path, language="ko", verbose=True)
        
        ids, docs, metadatas = [], [], []
        
        from tqdm import tqdm
        print(f"[KnowledgeBase] Indexing {len(result['segments'])} segments into ChromaDB...")
        
        # Batch Size Reduced to 10 to avoid Rate Limits (Free Tier)
        BATCH_SIZE = 10
        
        for i, seg in enumerate(tqdm(result['segments'], desc="Indexing")):
            text = seg['text'].strip()
            if len(text) < 2: continue
            
            ids.append(f"{video_id}_{i}")
            docs.append(text)
            metadatas.append({
                "video_id": video_id,
                "start": float(seg['start']),
                "end": float(seg['end'])
            })
            
            if len(ids) >= BATCH_SIZE:
                self._add_batch(ids, docs, metadatas)
                ids, docs, metadatas = [], [], []
                # Local Embedding: No sleep needed

        if ids:
            self._add_batch(ids, docs, metadatas)
        print(f"[KnowledgeBase] Ingest Complete for {video_id}.")

    def _add_batch(self, ids, docs, metadatas):
        import time
        max_retries = 5
        base_delay = 10
        
        for attempt in range(max_retries):
            try:
                self.collection.add(documents=docs, metadatas=metadatas, ids=ids)
                return # Success
            except Exception as e:
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    delay = base_delay * (attempt + 1)
                    print(f"\n[KB] âš ï¸ Rate Limit (429) hit. Sleeping for {delay}s... (Attempt {attempt+1}/{max_retries})")
                    time.sleep(delay)
                else:
                    print(f"\n[KB] âŒ DB Insert Error: {e}")
                    raise e
        print("\n[KB] âŒ Failed to insert batch after retries due to Rate Limits.")
        raise RuntimeError("ChromaDB Insert Failed: Rate Limit Exceeded")

    def get_context(self, video_id, start_time, end_time):
        """íŠ¹ì • ì‹œê°„ ë²”ìœ„ ë‚´ì˜ ëŒ€ë³¸ ì¶”ì¶œ (ìˆ˜ì • ì™„ë£Œ)"""
        results = self.collection.get(
            where={
                "$and": [
                    {"video_id": video_id},
                    {"start": {"$gte": float(start_time)}},
                    {"start": {"$lte": float(end_time)}}
                ]
            }
        )
        
        segments = []
        if results['ids']:
            for i in range(len(results['ids'])):
                segments.append({
                    "text": results['documents'][i],
                    "start": results['metadatas'][i]['start']
                })
        return sorted(segments, key=lambda x: x['start'])

    def clean_text_for_llm(self, text):
        text = re.sub(r'\[.*?\]', '', text)
        return re.sub(r'\s+', ' ', text).strip()

    def get_optimized_transcript(self, video_path, threshold_percentile=80):
        """[Token Saver] V1 ì˜¤ë””ì˜¤ ë¶„ì„ê³¼ ì—°ë™í•˜ì—¬ ëŒ€ë³¸ ì••ì¶•"""
        # 1. ë¡œì»¬ V1 ë¶„ì„ê¸° ê°€ë™
        try:
            from modules.analyst import Analyst
        except ImportError:
            print("[KB] âš ï¸ V1 Analyst module not found. Returning full text.")
            return "V1 Analyst module error."

        print("[KB] ğŸ“‰ Running 'Token Saver' Pre-filtering (Audio Analysis)...")
        
        # Load Khan Preset Manually
        import json
        preset_path = Path("presets/Khan.json")
        if preset_path.exists():
            with open(preset_path, "r", encoding="utf-8") as f:
                chk_config = json.load(f)
        else:
            print("[KB] âš ï¸ 'presets/Khan.json' not found. Using default config.")
            chk_config = {}

        analyst = Analyst(config=chk_config)
        audio_data = analyst.analyze_audio_advanced(video_path)
        
        if not audio_data:
            print("[KB] âš ï¸ Audio analysis failed. Falling back to FULL transcript.")
            return self.get_full_transcript(video_path)
            
        scores, times = analyst.calculate_scores(audio_data)
        threshold = np.percentile(scores, threshold_percentile)
        active_indices = np.where(scores > threshold)[0]
        
        if len(active_indices) == 0: return "No active zones found."
            
        # 2. í™œì„± êµ¬ê°„ ë³‘í•© (Peak +/- 3ë¶„)
        active_times = times[active_indices]
        ranges = sorted([(max(0, t - 180), t + 180) for t in active_times])
        
        merged = []
        if ranges:
            curr_start, curr_end = ranges[0]
            for start, end in ranges[1:]:
                if start < curr_end:
                    curr_end = max(curr_end, end)
                else:
                    merged.append((curr_start, curr_end))
                    curr_start, curr_end = start, end
            merged.append((curr_start, curr_end))
            
        # 3. ë°ì´í„° ì¶”ì¶œ ë° 30ì´ˆ ë‹¨ìœ„ ë¸”ë¡í™”
        video_id = Path(video_path).stem
        optimized_lines = []
        
        for start, end in merged:
            segs = self.get_context(video_id, start, end)
            
            # 30ì´ˆ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¬¶ê¸° (Text Slimming)
            current_block_id = -1
            for s in segs:
                block_id = int(s['start'] // 30)
                clean_txt = self.clean_text_for_llm(s['text'])
                if not clean_txt: continue
                
                if block_id != current_block_id:
                    m, sec = divmod(block_id * 30, 60)
                    h, m = divmod(m, 60)
                    timestamp = f"[{int(h):02d}:{int(m):02d}:{int(sec):02d}]"
                    optimized_lines.append(f"\n{timestamp} {clean_txt}")
                    current_block_id = block_id
                else:
                    optimized_lines[-1] += f" {clean_txt}"

        return "".join(optimized_lines)

    def get_full_transcript(self, video_path):
        """ì €ì¥ëœ ëª¨ë“  ë¬¸ì¥ì„ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ ëŒ€ë³¸ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤ (Fallbackì „ìš©)"""
        video_id = Path(video_path).stem
        print(f"[KnowledgeBase] ğŸ“ Fetching all segments for {video_id}...")
        results = self.collection.get(
            where={"video_id": video_id}
        )
        
        if not results['ids']:
            return ""

        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        segments = []
        for i in range(len(results['ids'])):
            segments.append({
                "start": results['metadatas'][i]['start'],
                "text": results['documents'][i]
            })
        
        sorted_segs = sorted(segments, key=lambda x: x['start'])
        
        # 30ì´ˆ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ í…ìŠ¤íŠ¸ ì–‘ ìµœì í™”
        full_text = []
        current_block = -1
        for s in sorted_segs:
            block = int(s['start'] // 30)
            if block != current_block:
                m, sec = divmod(block * 30, 60)
                h, m = divmod(m, 60)
                full_text.append(f"\n[{int(h):02d}:{int(m):02d}:{int(sec):02d}] {s['text']}")
                current_block = block
            else:
                full_text[-1] += f" {s['text']}"
        
        return "".join(full_text)
