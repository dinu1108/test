import os
import json
import re
import time
from pathlib import Path
from google import genai
from google.genai import types
from config import load_api_key, get_all_api_keys
from presets.factory_config import FactoryConfig

from .rejection_analyst import RejectionAnalyst

class LLMInterface:
    def __init__(self):
        load_api_key()
        self.api_keys = get_all_api_keys()
        self.current_key_idx = 0
        self.model_name = "gemini-1.5-flash-latest" # âœ… V5: 404 í•´ê²°ì„ ìœ„í•œ Fallback ID
        self._initialize_client()
        self.ra = RejectionAnalyst()

    def _initialize_client(self):
        """âœ… FIX: í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë¶„ë¦¬"""
        if not self.api_keys:
            raise ValueError("No API keys available!")
        self.client = genai.Client(api_key=self.api_keys[self.current_key_idx])

    def _rotate_api_key(self):
        """âœ… FIX: API í‚¤ ë¡œí…Œì´ì…˜ êµ¬í˜„"""
        if len(self.api_keys) > 1:
            self.current_key_idx = (self.current_key_idx + 1) % len(self.api_keys)
            self._initialize_client()
            print(f"[LLM] ğŸ”„ Switched to API key #{self.current_key_idx + 1}")
            return True
        return False

    def evaluate_candidates(self, kb, video_id, candidates, force_refresh=False):
        """
        [2ë‹¨ê³„: í‰ê°€ì ì—­í• ]
        í›„ë³´ ì»·ì˜ ì „í›„ ë§¥ë½ì„ ì¡°íšŒí•˜ì—¬ ì •ë°€ í‰ê°€ (V5: ì²´í¬í¬ì¸íŠ¸ ë¡œì§ ë³µêµ¬ + ë¦¬í”„ë ˆì‹œ ì˜µì…˜).
        """
        ckpt_file = Path("./hybrid_agent_v2/chroma_db") / f"{video_id}_ckpt.json"
        evaluations = []
        start_idx = 0

        if not force_refresh and ckpt_file.exists():
            try:
                with open(ckpt_file, "r", encoding="utf-8") as f:
                    evaluations = json.load(f)
                start_idx = len(evaluations)
                print(f"   ğŸ”„ Resuming from checkpoint: {start_idx}/{len(candidates)} completed.")
                if start_idx >= len(candidates): return {"evaluations": evaluations}
            except Exception as e:
                print(f"   âš ï¸ Checkpoint load failed: {e}")

        print(f"[LLM] âš–ï¸ {len(candidates)}ê°œì˜ í›„ë³´ ì»·ì— ëŒ€í•´ ì •ë°€ ì‹¬ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        for batch_start in range(start_idx, len(candidates), 10):
            batch = candidates[batch_start:batch_start + 10]
            # ... (Context gathering code stays similar)
            
            # Gather contexts for the batch
            batch_data = []
            for i, cand in enumerate(batch, start=batch_start):
                c_start = cand.get('start', cand.get('peak_time', 0) - 15)
                c_end = cand.get('end', cand.get('peak_time', 0) + 15)
                window = getattr(FactoryConfig, 'CONTEXT_WINDOW_SEC', 120)
                ctx_start = max(0, c_start - window)
                ctx_end = c_end + window
                context_docs = kb.get_context(video_id, ctx_start, ctx_end)
                context_text = " ".join([d['text'] for d in context_docs])
                
                batch_data.append({
                    "id": cand.get('id', i),
                    "start": c_start,
                    "end": c_end,
                    "text": cand.get('text', "No transcript"),
                    "context": context_text,
                    "speech_density": cand.get('speech_density', 0.5)
                })

            # âœ… FIX: í˜¸ì¶œ ê°„ê²© 2ì´ˆ (V5 ê°€ì´ë“œ)
            import time
            time.sleep(2)
            
            print(f"DEBUG: LLMInterface Current Model ID -> {self.model_name}")
            # Perform batch evaluation via LLM
            batch_results = self._evaluate_batch(batch_data)
            
            # âœ… FIX: ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜ë°›ì•„ ì²´í¬í¬ì¸íŠ¸ ì˜¤ì—¼ ë°©ì§€
            if batch_results is None:
                print(f"   âš ï¸ Batch processing failed. Stopping to preserve checkpoint integrity.")
                break

            evaluations.extend(batch_results)

            # âœ… FIX: V5 ì²´í¬í¬ì¸íŠ¸ ìƒì‹œ ì €ì¥
            try:
                ckpt_file = Path("./hybrid_agent_v2/chroma_db") / f"{video_id}_ckpt.json" # Re-define for scope
                ckpt_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ckpt_file, "w", encoding="utf-8") as f:
                    json.dump(evaluations, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"   âš ï¸ Failed to write checkpoint: {e}")

        return {"evaluations": evaluations}

    def _evaluate_batch(self, batch_data):
        """
        ì—¬ëŸ¬ í›„ë³´ë¥¼ í•œ ë²ˆì— LLMì— ë³´ë‚´ í‰ê°€ë°›ìŒ (ë¹„ìš© ë° ì‹œê°„ ì ˆê°)
        """
        from presets.factory_config import FactoryConfig
        style_desc = FactoryConfig.DESCRIPTION
        
        candidates_info = ""
        for item in batch_data:
            candidates_info += f"- ID {item['id']}: {item['start']:.1f}~{item['end']:.1f} / Text: {item['text']}\n"
            candidates_info += f"  Context: {item['context'][:1000]}...\n\n"

        prompt = f"""
# ROLE: ì„œì‚¬ ì¤‘ì‹¬ì˜ ì˜ìƒ ë‹¤íë©˜í„°ë¦¬ í¸ì§‘ì
# TASK: ì•„ë˜ ì œê³µëœ ì—¬ëŸ¬ í›„ë³´ ì»·ë“¤ì„ ë¶„ì„í•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê²¨ë¼.

# STYLE GUIDE: "{style_desc}"

# ğŸ§  LESSONS FROM THE PAST:
{self.ra.get_editing_feedback(limit=5)}

# CANDIDATES TO EVALUATE:
{candidates_info}

# EVALUATION CRITERIA (0~1.0):
1. emotion_intensity: ê°ì • í­ë°œ ì •ë„
2. info_density: ì„œì‚¬ì  ê°€ì¹˜ (ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ëŠ”ì§€ ì´í•´ ê°€ëŠ¥ ì—¬ë¶€)
3. narrative_payoff: ë¹Œë“œì—…ì— ëŒ€í•œ ë³´ìƒ (ì„±ê³µ/ì‹¤íŒ¨/ë°˜ì „ ë“±)
4. context_break: ë§¥ë½ ë‹¨ì ˆ (ë†’ì„ìˆ˜ë¡ ê°ì )
5. is_unnecessary: ë²„ë ¤ì•¼ í•  êµ¬ê°„ ì—¬ë¶€ (0 or 1)

# OUTPUT FORMAT (JSON Array of Objects):
[
  {{
    "id": 0,
    "emotion_intensity": 0.8,
    "info_density": 0.9,
    "narrative_payoff": 0.8,
    "context_break": 0.1,
    "is_unnecessary": 0,
    "reason": "í•œêµ­ì–´ í•œ ì¤„ í‰ê°€"
  }},
  ...
]
"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # âœ… FIX: ê°•ì œ í˜¸ì¶œ ê°„ê²© ë²Œë¦¬ê¸° (2ì´ˆ)
                time.sleep(2)
                
                print(f"DEBUG: LLMInterface Batch Current Model ID -> {self.model_name}")
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        temperature=0.2 # âœ… JSON ì¤€ìˆ˜ìœ¨ì„ ìœ„í•´ ì¡°ê¸ˆ ë” ë‚®ì¶¤
                    )
                )
                results = json.loads(response.text)
                
                # ì„±ê³µ ì‹œ ì¦‰ì‹œ ë°˜í™˜ ì²˜ë¦¬
                results_dict = {res['id']: res for res in results if 'id' in res}
                final_results = []
                for item in batch_data:
                    res = results_dict.get(item['id'], {
                        "emotion_intensity": 0, "info_density": 0, 
                        "narrative_payoff": 0, "context_break": 1.0, "is_unnecessary": 1,
                        "reason": "í‰ê°€ ëˆ„ë½"
                    })
                    res['id'] = item['id']
                    res['speech_density'] = item.get('speech_density', 0.5)
                    if 'reason' not in res: res['reason'] = "í‰ê°€ ë‚´ìš© ì—†ìŒ"
                    final_results.append(res)
                return final_results
                
            except Exception as e:
                error_msg = str(e)
                print(f"   âš ï¸ Batch evaluation error (Attempt {attempt+1}/{max_retries}): {error_msg}")
                
                if "429" in error_msg or "quota" in error_msg.lower():
                    if self._rotate_api_key():
                        continue
                
                if attempt < max_retries - 1:
                    time.sleep(5) # ì¬ì‹œë„ ì‹œì—ëŠ” ì¡°ê¸ˆ ë” ê¸¸ê²Œ ëŒ€ê¸°
        
        # âœ… FIX: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ None ë¦¬í„´ (ì´ìœ ë¥¼ ì„ì§€ ì•ŠìŒ)
        return None

    def _evaluate_single(self, candidate, context_text):
        """
        ë‹¨ì¼ í›„ë³´ í‰ê°€ (ì•ˆì „ì„± ê°•í™”)
        """
        from presets.factory_config import FactoryConfig
        
        # âš ï¸ [FIX] ë‹¤ì¤‘ í‚¤ ì²´ì¸ ë°©ì–´ (start -> peak_time -> ê¸°ë³¸ê°’ 0)
        c_start = candidate.get('start') or candidate.get('peak_time', 0) - 15
        c_end = candidate.get('end') or candidate.get('peak_time', 0) + 15
        c_text = candidate.get('text', "No transcript")
        style_desc = FactoryConfig.DESCRIPTION
        
        # ì‹œê°„ ê°’ì´ ìŒìˆ˜ê°€ ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
        c_start = max(0, float(c_start))
        c_end = max(c_start + 1, float(c_end))  # ìµœì†Œ 1ì´ˆ ê¸¸ì´ ë³´ì¥
        
        prompt = f"""
# ROLE: ì„œì‚¬ ì¤‘ì‹¬ì˜ ì˜ìƒ ë‹¤íë©˜í„°ë¦¬ í¸ì§‘ì
# TASK: ë‹¨ìˆœíˆ ì‹œë„ëŸ¬ìš´ êµ¬ê°„ì´ ì•„ë‹ˆë¼, 'ì´ì•¼ê¸°ì˜ ê²°ì‹¤'ì´ ìˆëŠ” êµ¬ê°„ì„ ì°¾ì•„ë¼.

# ì‹œì²­ìê°€ ì›í•˜ëŠ” 'ì •ë³´(Information)'ì˜ ì •ì˜:
1. ë¹Œë“œì—…ì˜ ë: ê³ ìƒí•˜ë˜ ë¯¸ì…˜ì„ ë§ˆì¹¨ë‚´ ì„±ê³µí•˜ê±°ë‚˜ í—ˆë¬´í•˜ê²Œ ì‹¤íŒ¨í•˜ëŠ” 'ê²°ê³¼'ê°€ ìˆëŠ”ê°€?
2. ë°˜ì „ì˜ ìˆœê°„: í‰ì˜¨í•˜ë‹¤ê°€ ê°‘ìê¸° ì˜ˆìƒì¹˜ ëª»í•œ ì‚¬ê±´(ê°‘íˆ­íŠ€, ë²„ê·¸, ë°°ì‹ )ì´ í„°ì§€ëŠ”ê°€?
3. ê°ì •ì˜ ê·¼ê±°: ë¹„ëª…ì„ ì§€ë¥¸ë‹¤ë©´ ê·¸ ì´ìœ ê°€ ëŒ€ë³¸ ìƒì— ëª…í™•íˆ ë“œëŸ¬ë‚˜ëŠ”ê°€? (ì´ìœ  ì—†ëŠ” ë¹„ëª…ì€ ê°ì )

# STYLE GUIDE: "{style_desc}"

# ğŸ§  LESSONS FROM THE PAST (Self-Correction):
{self.ra.get_editing_feedback(limit=5)}
(ìœ„ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì´ë²ˆ í‰ê°€ì—ì„œëŠ” ë” ì •êµí•œ ì•ˆëª©ì„ ì ìš©í•˜ë¼)

# CANDIDATE INFO:
- Time: {c_start:.1f} ~ {c_end:.1f}
- Transcript: {c_text}

# SURROUNDING CONTEXT (Â±2 min):
{context_text}

# EVALUATION CRITERIA (0~1.0):
1. emotion_intensity: ì›ƒìŒ, ë¶„ë…¸, ê°íƒ„ ë“± ê°ì •ì´ í­ë°œí•˜ëŠ”ê°€?
2. info_density: (ì¤‘ìš”) ì´ ì»·ë§Œ ë´ë„ 'ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ëŠ”ì§€' ì´í•´í•  ìˆ˜ ìˆëŠ”ê°€? ì„œì‚¬ì  ê°€ì¹˜ê°€ ìˆëŠ”ê°€?
3. narrative_payoff: ì•ì„  ìƒí™©ì— ëŒ€í•œ ë³´ìƒ(ì„±ê³µ/ì‹¤íŒ¨/ì›ƒìŒ í¬ì¸íŠ¸)ì´ í™•ì‹¤í•œê°€?
4. context_break: ì•ë’¤ ë§¥ë½ ì—†ì´ ê°‘ìê¸° íŠ€ì–´ë‚˜ì™€ì„œ ì´í•´í•˜ê¸° ì–´ë ¤ìš´ê°€? (ë†’ì„ìˆ˜ë¡ ë‚˜ì¨)
5. is_unnecessary: ë¡œë”© í™”ë©´, ë¬´ì˜ë¯¸í•œ ì¡ë‹´ ë“± ë²„ë ¤ì•¼ í•  êµ¬ê°„ì¸ê°€?

# OUTPUT FORMAT (JSON Only):
{{
  "emotion_intensity": 0.8,
  "info_density": 0.9,
  "narrative_payoff": 0.8,
  "context_break": 0.1,
  "is_unnecessary": false,
  "reason": "í•œêµ­ì–´ë¡œ ì§¤ë§‰í•œ í‰ê°€ (ì •ë³´/ì„œì‚¬ì„± ìœ„ì£¼ë¡œ ê¸°ìˆ )"
}}
"""
        try:
            # âœ… FIX: í˜¸ì¶œ ê°„ê²© 2ì´ˆ (V5 ê°€ì´ë“œ)
            time.sleep(2)
            
            print(f"DEBUG: LLMInterface Single Current Model ID -> {self.model_name}")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.3  # í‰ê°€ ì¼ê´€ì„± í–¥ìƒ
                )
            )
            result = self._safe_parse_json(response.text)
            
            # í•„ìˆ˜ í‚¤ ë³´ì¥ (Default Key Merging)
            defaults = {
                "emotion_intensity": 0.0,
                "info_density": 0.0,
                "narrative_payoff": 0.0,
                "context_break": 1.0,
                "is_unnecessary": False,
                "reason": "í‰ê°€ ì‹¤íŒ¨"
            }
            return {**defaults, **result}  # ëˆ„ë½ëœ í‚¤ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            
        except Exception as e:
            print(f"ğŸš¨ í‰ê°€ ì‹¤íŒ¨ (candidate ID: {candidate.get('id', '?')}): {e}")
            return {
                "emotion_intensity": 0,
                "info_density": 0,
                "narrative_payoff": 0,
                "context_break": 1.0,
                "is_unnecessary": False,
                "reason": f"API Error: {str(e)[:50]}"
            }

    def _safe_parse_json(self, raw_text):
        if not raw_text: return {"emotion_intensity": 0}
        try:
            return json.loads(raw_text)
        except:
            match = re.search(r"\{.*\}", raw_text, re.DOTALL)
            return json.loads(match.group()) if match else {"emotion_intensity": 0}