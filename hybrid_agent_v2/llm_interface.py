import os
import json
import re
import time
from google import genai
from google.genai import types
from config import load_api_key, get_all_api_keys

class LLMInterface:
    def __init__(self):
        load_api_key()
        self.api_keys = get_all_api_keys()
        self.current_key_idx = 0
        self.model_name = "models/gemini-2.5-flash"
        self.client = genai.Client(api_key=self.api_keys[0])

    def evaluate_candidates(self, kb, video_id, candidates):
        """
        [2ë‹¨ê³„: í‰ê°€ì ì—­í• ] 
        ê° í›„ë³´ ì»·ì˜ ì „í›„ ë§¥ë½ì„ KBì—ì„œ ì¡°íšŒí•˜ì—¬ ì •ë°€ í‰ê°€í•©ë‹ˆë‹¤.
        ì¤‘ê°„ ì €ì¥(Checkpoint) ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
        """
        import os
        from presets.factory_config import FactoryConfig
        
        print(f"[LLM] âš–ï¸ {len(candidates)}ê°œì˜ í›„ë³´ ì»·ì— ëŒ€í•´ ì •ë°€ ì‹¬ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Contextual Evaluation)...")
        
        # Checkpoint Load
        ckpt_file = FactoryConfig.CHECKPOINT_FILE
        evaluations = []
        start_idx = 0
        
        if os.path.exists(ckpt_file):
            try:
                with open(ckpt_file, "r", encoding="utf-8") as f:
                    evaluations = json.load(f)
                start_idx = len(evaluations)
                print(f"   ğŸ”„ Resuming from checkpoint: {start_idx}/{len(candidates)} completed.")
            except Exception as e:
                print(f"   âš ï¸ Checkpoint load failed, starting fresh: {e}")

        for i in range(start_idx, len(candidates)):
            cand = candidates[i]
            
            # 1. Context Fetching (Â±2ë¶„)
            # get_context args: video_id, start, end. We expand range here
            ctx_start = max(0, cand['start'] - FactoryConfig.CONTEXT_WINDOW_SEC)
            ctx_end = cand['end'] + FactoryConfig.CONTEXT_WINDOW_SEC
            
            context_docs = kb.get_context(video_id, ctx_start, ctx_end)
            context_text = " ".join([d['text'] for d in context_docs])
            
            # 2. Single Evaluation Prompt
            eval_result = self._evaluate_single(cand, context_text)
            
            # Map ID correctly
            eval_result['id'] = cand['id'] # Ensure ID matches
            evaluations.append(eval_result)
            
            # 3. Checkpoint Save (Every 1 cut or 5 cuts? 1 is safer for expensive LLM)
            with open(ckpt_file, "w", encoding="utf-8") as f:
                json.dump(evaluations, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… Evaluated #{i} (Score E:{eval_result.get('emotion_intensity',0)})")
            
        return {"evaluations": evaluations}

    def _evaluate_single(self, candidate, context_text):
        prompt = f"""
# ROLE: ëƒ‰í˜¹í•œ ì˜ìƒ ë¶„ì„ê°€
# TASK: ì•„ë˜ í›„ë³´ ì»·(CANDIDATE)ì´ ì‹œì²­ìì—ê²Œ ì¦ê±°ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ”ì§€ í‰ê°€í•˜ë¼.
# CONTEXT: í›„ë³´ ì»·ì˜ ì „í›„ 2ë¶„ ëŒ€ë³¸ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ë§¥ì„ íŒŒì•…í•˜ë¼.

# CANDIDATE INFO:
- Time: {candidate['start']} ~ {candidate['end']}
- Transcript: {candidate['text']}

# SURROUNDING CONTEXT (Â±2 min):
{context_text}

# EVALUATION CRITERIA (0~1.0):
1. emotion_intensity: (ì¤‘ìš”) ì›ƒìŒ, ë¶„ë…¸, ê°íƒ„ ë“± ê°ì •ì´ í­ë°œí•˜ëŠ”ê°€?
2. info_density: ìœ ìš©í•œ ì •ë³´ë‚˜ í†µì°°ì´ ìˆëŠ”ê°€?
3. context_break: ì•ë’¤ ë§¥ë½ ì—†ì´ ê°‘ìê¸° íŠ€ì–´ë‚˜ì™€ì„œ ì´í•´í•˜ê¸° ì–´ë ¤ìš´ê°€? (ë†’ì„ìˆ˜ë¡ ë‚˜ì¨)
4. is_unnecessary: ë¡œë”© í™”ë©´, ë¬´ì˜ë¯¸í•œ ì¡ë‹´ ë“± ë²„ë ¤ì•¼ í•  êµ¬ê°„ì¸ê°€?

# OUTPUT FORMAT (JSON Only):
{{
  "emotion_intensity": 0.8,
  "info_density": 0.5,
  "context_break": 0.1,
  "is_unnecessary": false,
  "reason": "í•œêµ­ì–´ë¡œ ì§¤ë§‰í•œ í‰ê°€ (ì´ìœ )"
}}
"""
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return self._safe_parse_json(response.text)
        except Exception as e:
            print(f"ğŸš¨ í‰ê°€ ì‹¤íŒ¨ (ID: {candidate.get('id')}): {e}")
            return {} # Return empty dict, will be handled as missing score

    def _safe_parse_json(self, raw_text):
        try:
            return json.loads(raw_text)
        except:
            match = re.search(r"\{.*\}", raw_text, re.DOTALL)
            return json.loads(match.group()) if match else {"evaluations": []}